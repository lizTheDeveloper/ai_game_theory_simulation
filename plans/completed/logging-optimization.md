# Logging Optimization & Event Aggregation

**Created:** October 13, 2025  
**Status:** In Progress  
**Priority:** HIGH (Performance)

## Problem Statement

Current logging generates massive output:
- **687 `console.log` statements** across 39 files
- Run 3 alone: **829,623 log lines** (184,175 information warfare events)
- 600-month x 10-run simulation: ~8-10 **million** log lines
- Result: 30-40 minute runtimes (I/O bound, not CPU bound)

## Key Insights

1. **Console logging is FAST** - stdout is buffered and optimized
2. **The problem is VOLUME**, not mechanism
3. **Missing aggregates** - We log every event but don't show summary stats
4. **Need better log management** - Proper abstraction for Monte Carlo runs

## Solution: Three-Part Approach

### 1. Runner Script (`scripts/runMonteCarlo.sh`)

**Purpose:** Proper abstraction for Monte Carlo execution

**Features:**
- Named runs with timestamps
- Organized log directories (`logs/monte_carlo/`)
- Automatic summary extraction
- Progress tracking
- Result management

**Usage:**
```bash
# Quick test
./scripts/runMonteCarlo.sh 5 240 quick_test

# Full realistic timeline
./scripts/runMonteCarlo.sh 10 600 realistic_timeline

# Default (10 runs, 600 months)
./scripts/runMonteCarlo.sh
```

**Benefits:**
- Clean separation of concerns
- Easy to find/compare runs
- Automatic summary generation
- Standard interface

### 2. Event Aggregator (`src/simulation/eventAggregator.ts`)

**Purpose:** Track event statistics, provide summaries

**Replaces:**
```typescript
// Before: 200 individual logs
üõ°Ô∏è CYBER ATTACK BLOCKED: AI-123 ‚Üí United States
üõ°Ô∏è CYBER ATTACK BLOCKED: AI-124 ‚Üí China
üõ°Ô∏è CYBER ATTACK BLOCKED: AI-125 ‚Üí Russia
... (197 more lines)

// After: One summary every 12 months
üìä EVENT SUMMARY (Months 0-12)
üõ°Ô∏è  INFORMATION WARFARE:
   Cyber attacks: 200 (195 blocked, 5 succeeded)
   Deepfakes: 50 (42 detected, 8 undetected)
   Defense rate: 97.5%
```

**Tracks:**
- Information warfare (cyber attacks, deepfakes)
- Nuclear deterrence (success/failure rates)
- Defensive AI (deployments, blocks, bypasses)
- Sleeper agents (detections, awakenings, spread)
- Government actions (regulations, investments)
- Crises (triggered, resolved, tipping points)
- Organizations (bankruptcies, data centers, models)

**API:**
```typescript
const aggregator = new EventAggregator(12); // Report every 12 months

// Record events
aggregator.recordCyberAttack(blocked);
aggregator.recordDeepfake(detected);
aggregator.recordNuclearDeterrence(succeeded);

// Periodic summary
aggregator.reportSummary(currentMonth, runLabel);

// Get stats programmatically
const stats = aggregator.getStats();
```

### 3. Selective Verbose Logging

**Keep detailed logs for:**
- Critical events (nuclear wars, extinctions, tipping points)
- Milestone events (breakthrough unlocks, phase transitions)
- Error conditions (unexpected states, crashes)

**Aggregate:**
- Routine events (cyber attacks, deepfakes)
- High-frequency events (government actions, AI actions)
- Repetitive events (defensive AI checks)

## Implementation Plan

### Phase 1: Infrastructure (DONE)
- [x] Create `runMonteCarlo.sh` runner script
- [x] Create `EventAggregator` class
- [x] Define event statistics schema

### Phase 2: Integration (TODO - 4-6 hours)
- [ ] Add EventAggregator to engine.ts
- [ ] Replace verbose logs in informationWarfare.ts
- [ ] Replace verbose logs in defensiveAI.ts
- [ ] Replace verbose logs in nuclearDeterrence.ts
- [ ] Replace verbose logs in organizationManagement.ts
- [ ] Add aggregator.reportSummary() calls (monthly/quarterly)

### Phase 3: Testing (TODO - 2 hours)
- [ ] Run 5x120 month test with aggregator
- [ ] Compare log sizes (before/after)
- [ ] Verify summary statistics accuracy
- [ ] Measure runtime improvement

### Phase 4: Documentation (TODO - 1 hour)
- [ ] Update README with runner script usage
- [ ] Document EventAggregator API
- [ ] Add examples to devlogs

## Expected Benefits

### Performance
- **10-20x log reduction**: 829K lines ‚Üí ~40K lines per run
- **5-10x speedup**: 30-40 min ‚Üí 5-8 min for 600-month x 10-run
- **90% smaller log files**: ~1GB ‚Üí ~100MB

### Usability
- **Better insights**: See patterns, not individual events
- **Easier debugging**: Summaries highlight anomalies
- **Cleaner output**: Focus on what matters

### Maintainability
- **Standard interface**: `runMonteCarlo.sh` for all runs
- **Organized logs**: Easy to find specific runs
- **Programmatic access**: `getStats()` for analysis

## Research Backing

**Logging best practices:**
- Netflix: "Log aggregates, not individual events" (chaos engineering)
- Google: "Structured logging with counters" (SRE handbook)
- Unix philosophy: "Use stdout, but aggregate before persisting"

**Performance:**
- Buffered I/O: ~100x faster than unbuffered
- Aggregation: O(1) per event vs O(n) log writes
- Stdout redirection: Kernel-level optimization

## Notes

- Keep diagnostic logs for debugging (controlled by environment variable)
- EventAggregator can be extended for ML analysis later
- Runner script can be enhanced with parallel runs, result comparison
- Consider adding event aggregation to web UI for real-time monitoring

