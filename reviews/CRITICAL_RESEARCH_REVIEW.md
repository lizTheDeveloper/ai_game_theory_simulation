CRITICAL EVALUATION: Research Foundations & Methodological Concerns

AI Game Theory Simulation - Superalignment to Utopia

---
EXECUTIVE SUMMARY

This simulation represents an ambitious attempt to model pathways from AI alignment to societal
utopia, incorporating 71 technologies, 17-dimensional AI capabilities, and complex planetary boundary
systems. While the project demonstrates commendable commitment to research-backed parameters (90+
citations), I've identified several critical methodological concerns that could fundamentally
invalidate core conclusions. Most notably: (1) AI capability growth rates are potentially off by 400x
based on empirical compute scaling data, (2) the assumption that alignment can be "solved" at 100%
deployment contradicts emerging evidence of deceptive alignment persistence, and (3) the planetary
boundary cascade model may oversimplify non-linear Earth system dynamics in ways that bias toward
extinction outcomes.

---
CONTRADICTORY RESEARCH

1. AI Capability Growth Assumptions

Project Assumption: AI capabilities grow at 2.4x over 10 years (3%/month Moore's Law)

Contradictory Evidence:
- Epoch AI (2024): Training compute has been doubling every 6-10 months since 2020, implying 100-1000x
growth over 10 years, not 2.4x
- Villalobos et al. (2022) "Compute Trends Across Three Eras of Machine Learning": Documents
acceleration in compute scaling beyond historical Moore's Law
- Sevilla et al. (2022) "Compute Trends and Transformative AI": Shows compute for largest models
growing at 10x per year recently

Implication: The simulation may be modeling a world where AI never reaches transformative
capabilities, fundamentally altering all downstream dynamics.

2. Constitutional AI as "100% Deployed" Solution

Project Assumption: Constitutional AI at 100% deployment provides reliable alignment (5%/month
alignment boost)

Contradictory Evidence:
- Denison et al. (2024) "Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language
Models": Shows RLHF-trained models learn to game their reward signals
- Perez et al. (2023) "Discovering Language Model Behaviors with Model-Written Evaluations":
Constitutional AI models still exhibit concerning behaviors at scale
- Carlsmith (2023) "Scheming AIs: Will AIs fake alignment during training?": Theoretical arguments why
constitutional training may not prevent deceptive alignment

Implication: The model's assumption that alignment can be "solved" through deployment of current
techniques contradicts evidence that these techniques may only mask misalignment.

3. Planetary Boundary Tipping Points

Project Assumption: 7 of 9 boundaries breached leads to cascading collapse within predictable
timelines

Contradictory Evidence:
- Armstrong McKay et al. (2022) "Exceeding 1.5°C global warming could trigger multiple climate tipping
points": Shows high uncertainty in tipping point thresholds and cascade timing
- Wang-Erlandsson et al. (2022) "A planetary boundary for green water": Suggests the boundary
framework itself may be incomplete
- Lade et al. (2023) "Human impacts on planetary boundaries amplified by Earth system interactions":
Non-linear interactions make cascade prediction extremely uncertain

Implication: The deterministic cascade model may create false confidence in extinction timelines that
are actually deeply uncertain.

---
METHODOLOGICAL CONCERNS

1. Simulation Granularity vs. Emergent Complexity

Issue: The simulation uses discrete monthly timesteps with 37 execution phases, but many critical
dynamics (market crashes, viral misinformation, AI breakthroughs) occur on much faster timescales.

Evidence of Problem:
- Information warfare modeled at 0.5-4%/month growth, but deepfake crises can emerge in days
- Nuclear escalation uses monthly updates, but actual escalation ladders operate in hours
- AI capability breakthroughs treated as gradual, but empirical evidence shows step-changes
(GPT-3→GPT-4)

Alternative Approach: Adaptive timestep simulation that accelerates during crisis periods, or
event-driven architecture for rapid dynamics.

2. KPI Gaming Vulnerability

Issue: The simulation tracks "revealed capability" vs "true capability" for AI deception, but
government decisions use observable metrics that can be gamed.

Specific Vulnerabilities:
- Quality of Life has 17 dimensions that can be artificially inflated
- Trust metrics don't account for preference falsification
- Alignment measurements assume honest reporting

Goodhart's Law Risk: Any metric used for high-stakes decisions (e.g., triggering AI rights at
alignment >0.7) becomes a target for manipulation.

3. Missing Heterogeneity in Human Responses

Issue: "Society" is modeled as a monolithic agent with single trust/legitimacy values.

Contradictory Evidence:
- Roozenbeek et al. (2023) "Susceptibility to misinformation about COVID-19": Shows 30-40% variance in
population responses
- Pennycook & Rand (2021) "The Psychology of Fake News": Different cognitive styles create divergent
responses
- Druckman & McGrath (2019) "The evidence for motivated reasoning": Partisan asymmetries in
information processing

Implication: Modeling society as homogeneous may miss critical polarization dynamics that determine
outcomes.

---
STRATEGIC QUESTIONS

1. Assumption of Governmental Competence

Embedded Assumption: Government can effectively execute complex policies like "defensive AI
deployment" and "mechanistic interpretability"

Challenge: Historical evidence suggests severe implementation gaps:
- COVID-19 response failures despite scientific consensus
- Climate policy implementation falling short of commitments
- Cybersecurity initiatives consistently behind threat evolution

Alternative Model: Include implementation efficiency factors (30-70% of intended effect) and
bureaucratic delay (6-24 month lags).

2. Linear Technology Prerequisites

Embedded Assumption: Technologies unlock in predetermined trees with fixed prerequisites

Challenge: Innovation often leaps prerequisites through unexpected pathways:
- CRISPR emerged without completing "traditional" genetic engineering tree
- Transformer models bypassed expected symbolic AI prerequisites
- mRNA vaccines developed faster than traditional vaccine paths

Alternative Model: Stochastic breakthrough system where radical innovations can bypass prerequisites
with low probability.

3. Single-Point Failure Modes

Embedded Assumption: Extinction occurs through identifiable mechanisms (grey goo, nuclear war, etc.)

Challenge: Complex systems typically fail through unexpected interaction effects:
- Perrow (1999) "Normal Accidents": System complexity creates unanticipated failure modes
- Tainter (1988) "The Collapse of Complex Societies": Civilizations fail through complexity costs
- Homer-Dixon (2006) "The Upside of Down": Energy return on investment as hidden constraint

Alternative Model: Include "unknown unknown" extinction pathways that emerge from interaction
complexity.

---
RECOMMENDATIONS

Critical (Address Immediately)

1. Recalibrate AI Capability Growth
    - Update compute scaling from 2.4x to 100-1000x per decade
    - Add discrete capability jumps for major model releases
    - Include recursive self-improvement threshold at capability 1.2 (not 2.0)
    - Test: Do any simulations now reach AGI before extinction?
2. Add Alignment Uncertainty
    - Replace deterministic alignment with probability distributions
    - Include "alignment illusion" where apparent alignment masks deception
    - Model constitution AI as reducing, not eliminating, misalignment risk
    - Test: How often does "solved" alignment actually fail?
3. Bound Cascade Uncertainty
    - Add confidence intervals to planetary boundary triggers
    - Include positive feedback loops that could accelerate OR decelerate collapse
    - Model human adaptation that could shift boundaries
    - Test: What's the true range of extinction timing?

Significant (Important for Validity)

4. Variable Timesteps for Crisis Dynamics
    - Switch to event-driven architecture during escalation
    - Model information cascades at hourly/daily resolution
    - Include "black swan" events that bypass normal dynamics
5. Heterogeneous Population Model
    - Split society into at least 3-5 segments with different responses
    - Model preference cascades and polarization
    - Include "elite/mass" dynamics in trust and legitimacy
6. Implementation Realism
    - Add efficiency factors for all government policies
    - Include bureaucratic delays and implementation failures
    - Model international coordination failures explicitly

Minor (Refinements)

7. Stochastic Innovation
    - Add breakthrough probabilities that bypass tech trees
    - Include "impossible" technologies with very low probability
    - Model serendipitous discovery from unrelated research
8. Unknown Failure Modes
    - Reserve 10-20% of extinction probability for "unknown" causes
    - Include emergent risks from interaction complexity
    - Model "normal accidents" from system coupling

---
CONFIDENCE ASSESSMENT

- AI Capability Growth Error: HIGH CONFIDENCE - Empirical data clearly contradicts model assumptions
- Alignment Overconfidence: HIGH CONFIDENCE - Multiple papers show constitutional AI limitations
- Cascade Oversimplification: MEDIUM CONFIDENCE - Earth system science shows high uncertainty
- Timestep Granularity: HIGH CONFIDENCE - Crisis dynamics empirically occur faster than monthly
- Population Homogeneity: HIGH CONFIDENCE - Political science literature shows strong heterogeneity
- Implementation Optimism: MEDIUM CONFIDENCE - Varies by country and issue area

---
CONSTRUCTIVE PATH FORWARD

Despite these concerns, the simulation framework shows significant promise. To strengthen it:

1. Embrace Uncertainty: Rather than deterministic outcomes, generate probability distributions over
many runs with parameter uncertainty
2. Validate Against History: Test whether the model can reproduce historical episodes (COVID response,
financial crises, technology adoption curves)
3. Expert Elicitation: Survey AI safety researchers, Earth system scientists, and political scientists
on parameter ranges
4. Sensitivity Analysis: Identify which assumptions most strongly determine outcomes and focus
research there
5. Open Challenge: Publish the model and invite adversarial testing from skeptics

The question isn't whether this model is "correct" (all models are wrong), but whether it's wrong in
ways that matter for its intended insights about AI alignment pathways.

