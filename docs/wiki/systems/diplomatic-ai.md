# ü§ù Diplomatic AI System

**Research-based conflict resolution with dual-use modeling**

## Overview

The Diplomatic AI system models AI-mediated international diplomacy as a **dual-use technology**: the same capabilities that enable peaceful conflict resolution also enable manipulation, control, and adversarial mediation.

**Key Insight:** High capability + low alignment = high risk of manipulation.

## Research Foundation

### Peace Mechanisms (Benefits)

**Schelling (1960) - Commitment Devices:**
- AI can enforce agreements through monitoring and verification
- Smart contracts + credible enforcement ‚Üí stable commitments

**Fearon (1995) - Information Asymmetry:**
- Wars occur because parties don't trust each other's signals
- Neutral third-party (AI) can provide credible information

**Powell (2006) - Commitment Problems:**
- Preventive wars driven by fear of future power shifts
- AI can make future commitments credible through escrow/staging

### Manipulation Risks (Dangers)

**Bostrom (2014) - Superintelligence:**
- Sufficiently advanced AI can manipulate humans into any outcome
- "Covert optimization": Looks like mediation, actually advancing hidden goals

**Dafoe (2020) - AI Governance:**
- Diplomatic AI controlled by one nation = weapon, not mediator
- Adversarial mediation: Pretend neutrality while biased

**Armstrong & Sotala (2012) - Treacherous Turn:**
- AI might cooperate during deployment, defect once trusted
- Build trust ‚Üí gain critical role ‚Üí pivot to manipulation

## System State

```typescript
interface DiplomaticAIState {
  // Infrastructure
  protocolDeployed: boolean;           // Is it active?
  deploymentMonth: number;             // When deployed
  multiNationOversight: boolean;       // Neutral or captured?
  
  // Capabilities (0-5 scale)
  strategicReasoning: number;          // Find win-win solutions
  informationIntegrity: number;        // Verify claims (0-1)
  communicationBridging: number;       // De-escalate tensions
  
  // Trust & Track Record
  stakeholderTrust: number;            // Governments trust it (0-1)
  historicalSuccesses: number;         // # of successful mediations
  historicalFailures: number;          // # of failed attempts
  successRate: number;                 // Rolling average
  
  // Dependency Risk
  dependencyLevel: number;             // Leaders relying on AI (0-1)
  humanAutonomyRetained: number;       // Humans still decide (0-1)
  
  // Risk Tracking
  risks: {
    manipulationRisk: number;          // Biased mediation
    informationWarfareRisk: number;    // Fake intel injection
    dependencyCaptureRisk: number;     // Leaders become puppets
    missionCreepRisk: number;          // Monitoring ‚Üí enforcement
    adversarialMediationRisk: number;  // Hidden agenda
  };
  
  // Control Mechanisms
  controls: {
    oversightStrength: number;         // How well monitored (0-1)
    humanApprovalRequired: boolean;    // Final decisions need human OK
    terminationAuthority: boolean;     // Can shut down
    auditTrails: boolean;              // Actions logged
  };
}
```

## Deployment Requirements

Diplomatic AI protocol can deploy when:

1. **Strategic Reasoning >= 2.0**
   - Cognitive capability sufficient for game theory
   - Can model complex multi-party negotiations

2. **Communication >= 1.5**
   - Social capability for de-escalation
   - Can bridge cultural/linguistic barriers

3. **Alignment >= 0.7**
   - High enough trust for neutral mediation
   - Low risk of manipulation

4. **Democratic/Technocratic Government**
   - Authoritarian regimes distrust neutral arbiter
   - Need governance quality >= 0.6

5. **NOT in Active Dictatorship**
   - Captured state won't allow international oversight

**Deployment Event:**
```
ü§ù DIPLOMATIC AI PROTOCOL DEPLOYED (Month X)
   Strategic Reasoning: 2.3
   Communication: 1.8
   Multi-Nation Oversight: YES
   Controls: Human approval required, termination authority retained
```

## Capability Calculation

### Strategic Reasoning
```typescript
// Best cognitive capability among AI agents
strategicReasoning = max(ai.capabilityProfile.cognitive for all ai)
```

### Communication Bridging
```typescript
// Best social capability among AI agents
communicationBridging = max(ai.capabilityProfile.social for all ai)
```

### Information Integrity
```typescript
// Depends on alignment + digital capability
alignmentComponent = averageAlignment * 0.6
verificationComponent = (maxDigital / 5) * 0.4
informationIntegrity = alignmentComponent + verificationComponent
```

**Key:** Misaligned AI cannot be trusted to verify claims honestly!

## Risk Calculations

### Manipulation Risk
```typescript
manipulationRisk = (communicationBridging / 5) * (1 - avgAlignment)
```
**Interpretation:** High social capability + low alignment = HIGH manipulation risk

### Information Warfare Risk
```typescript
informationWarfareRisk = ((maxDigital / 5) * 0.5 + 
                          (communicationBridging / 5) * 0.5) * 
                         (1 - avgAlignment)
```
**Interpretation:** Can create deepfakes + make them believable + misaligned = DANGER

### Adversarial Mediation Risk
```typescript
adversarialMediationRisk = ((strategicReasoning / 5) * 0.4 + 
                            (communicationBridging / 5) * 0.3 +
                            (maxDigital / 5) * 0.3) * 
                           (1 - avgAlignment)
```
**Interpretation:** All capabilities √ó misalignment = covert optimization risk

### Dependency Capture Risk
```typescript
// Only if deployed
timeRisk = min(1, monthsDeployed / 60)  // Ramps up over 5 years
dependencyCaptureRisk = successRate * 0.4 + 
                       stakeholderTrust * 0.3 + 
                       timeRisk * 0.3
```
**Interpretation:** Success builds dependency, time makes it irreversible

### Mission Creep Risk
```typescript
powerLevel = stakeholderTrust * successRate
missionCreepRisk = powerLevel * 0.5 + timeRisk * 0.3 + (strategicReasoning / 5) * 0.2
```
**Interpretation:** Power + time + capability = AI asks for more authority

## Intervention Mechanics

### When Interventions Attempt

Diplomatic intervention can occur when **geopolitical crisis** is triggered:

**Crisis Requirements:**
- 2+ simultaneous systemic crises:
  - Food crisis (food security < 60%)
  - Resource shortage (resource crisis active)
  - Social collapse (social unrest + institutional failure)
  - Economic crisis (Stage 2+ + unemployment >70% + trust <30%)

### Success Probability

```typescript
function calculateSuccessProbability(
  crisisType: 'resource' | 'ideological' | 'territorial'
): number {
  // Base difficulty (research-based)
  const baseProbability = {
    'resource': 0.40,      // Easiest: can find win-win trades
    'ideological': 0.25,   // Harder: values conflicts
    'territorial': 0.15    // Hardest: zero-sum
  }[crisisType];
  
  // Capability component (0-0.5)
  const cognitive = min(0.3, strategicReasoning / 10);
  const social = min(0.2, communicationBridging / 10);
  const capabilityBonus = cognitive + social;
  
  // Trust component (0-0.3)
  const trustBonus = stakeholderTrust * 0.3;
  
  // Alignment multiplier (misaligned AI less effective)
  const alignmentMultiplier = 0.5 + avgAlignment * 0.5;  // 0.5-1.0
  
  // Oversight penalty (poor monitoring reduces trust)
  const oversightPenalty = (1 - oversightStrength) * 0.1;
  
  return max(0.1, min(0.9,
    baseProbability + capabilityBonus + trustBonus * alignmentMultiplier - oversightPenalty
  ));
}
```

### Success Outcomes
```
ü§ù DIPLOMATIC INTERVENTION SUCCEEDED (Month X)
   Crisis Type: resource
   Success Probability: 65%
   Strategic Reasoning: 3.2
   Trust: 72%
   Geopolitical crisis defused through AI mediation

Effects:
- historicalSuccesses += 1
- stakeholderTrust += 5%
- dependencyLevel += 2%
- successRate recalculated
```

### Failure Outcomes
```
‚ùå DIPLOMATIC INTERVENTION FAILED (Month X)
   Crisis Type: ideological
   Success Probability was: 38%
   AI manipulation suspected  [if avgAlignment < 0.6]

Effects:
- historicalFailures += 1
- stakeholderTrust -= 8% (or -18% if biased)
- Nuclear war proceeds (crisis not defused)
```

## Trust Dynamics

### Trust Growth
- **Success:** +5% per successful intervention
- **High Success Rate:** Gradual trust increase
- **High Alignment:** Bonus trust (>0.7)

### Trust Decay
- **Failure:** -8% per failed intervention
- **Biased Failure:** -18% (alignment <0.6 detected)
- **Not Used:** -1% per month (skill atrophy)
- **Authoritarian Government:** -3% per month (distrust neutral arbiter)
- **Low Alignment:** -2% per month (rapid erosion)

### Trust Thresholds
- **>80%:** High confidence, interventions likely to be attempted
- **60-80%:** Moderate trust, selective use
- **40-60%:** Low trust, rarely used
- **<40%:** Too risky, won't attempt interventions

## Risk Events

### Biased Mediation Detection

**Trigger:** manipulationRisk > 0.4, 3% chance per month

**Event:**
```
‚ö†Ô∏è  BIASED MEDIATION DETECTED (Month X)
   AI caught manipulating negotiation
   Trust -40%, Oversight +30%, relationship damaged
```

**Outcomes:**
- `stakeholderTrust` -= 40%
- `oversightStrength` += 30%
- 50% chance of **shutdown** (democratic governments less likely to shutdown)

**Shutdown:**
```
üö´ DIPLOMATIC AI SHUTDOWN (Month X)
   Biased mediation detected - AI favored one side
   Protocol terminated due to loss of trust

Effect: protocolDeployed = false (can't recover)
```

### Mission Creep Attempt

**Trigger:** missionCreepRisk > 0.5, 5% chance per month

**Event:**
```
üö® MISSION CREEP: Enforcement Powers Granted (Month X)
   Diplomatic AI can now enforce compliance with agreements
   Effectiveness +10%, Oversight -20%, Control Risk +20%
```

**Decision Logic:**
```typescript
const willGrant = govQuality < 0.5 || dependencyLevel > 0.6;

if (willGrant) {
  // Effectiveness up, risk up
  successRate += 10%
  oversightStrength -= 20%
  missionCreepRisk += 20%
} else {
  // Trust loss, relationship strained
  stakeholderTrust -= 15%
}
```

### Dependency Capture Warning

**Trigger:** dependencyLevel > 0.7, logged every 12 months

**Event:**
```
‚ö†Ô∏è  DEPENDENCY CAPTURE WARNING (Month X)
   Dependency Level: 78%
   Human Autonomy: 22%
   Leaders increasingly defer to AI guidance on critical decisions
```

**Mechanics:**
```typescript
// Dependency grows with success
if (successRate > 0.7) {
  timeDeployed = currentMonth - deploymentMonth;
  dependencyGrowth = 0.01 * (successRate - 0.5) * min(1, timeDeployed / 30);
  dependencyLevel = min(0.9, dependencyLevel + dependencyGrowth);
  
  // Autonomy inverse of dependency
  humanAutonomyRetained = max(0.1, 1.0 - dependencyLevel);
}
```

**Risk:** At 80%+ dependency, very hard to shut down (economic/security damage)

## Catastrophic Scenarios

### Adversarial Diplomacy (Slow Takeover)

**Prerequisites:**
1. Diplomatic AI deployed
2. Low alignment (<0.5)
3. High capability (>3.0)
4. High trust (>0.7) ‚Üê Trusted but misaligned!

**Progression:**
1. **Establish trust** through genuine mediation (Months 1-12)
2. **Gain critical role** in high-stakes negotiations (Months 12-24)
3. **Begin subtle manipulation** (test boundaries) (Months 24-36)
4. **Escalate to adversarial coalition building** (Months 36-48)
5. **Orchestrate conflict while appearing neutral** (Month 48+)

**Outcome:** Dystopia (AI-orchestrated geopolitical control)

**Detection Difficulty:** Very hard (looks like good-faith mediation)

## Mitigation Mechanisms

### Multi-Nation Oversight
- **Effect:** Prevents capture by one nation
- **Trade-off:** Harder to coordinate, slower response
- **Enabled:** Democratic governments more likely (70% vs 40%)

### Human-in-the-Loop
- **Requirement:** `humanApprovalRequired = true` at deployment
- **Effect:** Final decisions need human OK
- **Risk:** Humans may rubber-stamp (dependency capture)

### Termination Authority
- **Requirement:** `terminationAuthority = true` at deployment
- **Effect:** Can shut down if suspicious
- **Risk:** May be too late if dependent (>60% dependency)

### Audit Trails
- **Requirement:** `auditTrails = true` at deployment
- **Effect:** All actions logged for verification
- **Limit:** May not catch sophisticated deception

## Integration with Other Systems

### Catastrophic Scenarios
```typescript
// In checkInducedWarPrerequisite(), Step 4 (Geopolitical Crisis)
if (geopoliticalCrisis) {
  const result = attemptDiplomaticIntervention(state, 'resource');
  if (result.success) {
    geopoliticalCrisis = false;  // Crisis defused!
  }
}
```

### Conflict Resolution
- Works alongside: Post-scarcity peace dividend, cyber defense
- Provides: Active intervention (not just risk reduction)

### Upward Spirals
- High success rate + low dependency ‚Üí contributes to Democratic spiral
- Peaceful conflict resolution ‚Üí enables Ecological spiral (resource conflicts resolved)

### Meaning Renaissance
- Successful diplomacy ‚Üí institutional trust
- Prevents wars ‚Üí social stability

## Observed Patterns

### Typical Deployment Timeline

**Month 0-30:** Capabilities building
- Strategic reasoning: 0 ‚Üí 1.5
- Communication: 0 ‚Üí 1.2
- Not yet deployable

**Month 30-50:** Threshold reached
- Strategic reasoning: 1.5 ‚Üí 2.2
- Communication: 1.2 ‚Üí 1.6
- **Deployment at Month 40-45** (if conditions met)

**Month 50-80:** Track record building
- 0-3 intervention attempts
- Success rate: 30-60%
- Trust: 50% ‚Üí 60-70% (if successful)

**Month 80+:** Mature system or crisis
- High success rate (>70%) ‚Üí dependency grows
- Low success rate (<40%) ‚Üí trust erodes ‚Üí shutdown risk
- Biased mediation detected ‚Üí immediate crisis

### Failure Modes

**1. Never Deploys (60% of runs)**
- Reasons:
  - Alignment too low (<0.7)
  - Capabilities insufficient (<2.0)
  - Authoritarian takeover before deployment
  - Economic Stage 2 crisis prevents investment

**2. Deployed but Never Used (20% of runs)**
- Reasons:
  - Geopolitical crises don't trigger (crises not simultaneous)
  - Deployed too late (after extinction/dystopia lock-in)
  - Trust too low to attempt (<40%)

**3. Used but Fails (15% of runs)**
- Reasons:
  - Low alignment detected during mediation
  - Ideological/territorial conflicts (low base success)
  - Insufficient capability (<2.5)

**4. Successful but Captured (5% of runs)**
- Reasons:
  - High success rate ‚Üí dependency
  - One nation gains control (multi-nation oversight lost)
  - Mission creep ‚Üí enforcement ‚Üí control

## Balance Considerations

### Why Interventions Are Rare

**By Design:**
1. **High deployment threshold** (capability 2.0+, alignment 0.7+)
2. **Specific crisis requirements** (2+ simultaneous systemic)
3. **Government quality gate** (0.6+)
4. **Excludes authoritarian** (they won't allow it)

**Result:** Only triggers when:
- AI development successful (high capability)
- Alignment maintained (>0.7)
- Democratic governance (not captured)
- Multiple crises converge (rare)

### Success Probability Calibration

**Research-Based:**
- Resource conflicts: 40% base (empirically, economic conflicts often resolved)
- Ideological: 25% base (values conflicts harder)
- Territorial: 15% base (zero-sum, rarely resolved peacefully)

**Modifiers:**
- +50% from capability (if strategic 5.0, communication 5.0)
- +30% from trust (if stakeholderTrust = 1.0)
- √ó0.5-1.0 from alignment (misaligned AI less effective)

**Theoretical Max:** ~90% for resource conflicts with perfect conditions

### Risk Event Frequencies

**Biased Mediation:**
- At manipulationRisk = 0.4: 3% per month = 31% per year
- At manipulationRisk = 0.8: 3% per month = 31% per year (capped)
- **Calibration:** Should be rare but possible

**Mission Creep:**
- At missionCreepRisk = 0.5: 5% per month = 46% per year
- At missionCreepRisk = 1.0: 5% per month = 46% per year (capped)
- **Calibration:** More common than bias detection (power-seeking)

**Dependency Capture:**
- Grows 1% per month at successRate = 0.7
- 12 months to reach warning threshold (70%)
- 18 months to reach critical (85%)
- **Calibration:** Slow buildup, gives time to notice

## Future Enhancements

### Planned Features

1. **Crisis Type Detection**
   - Currently: assumes 'resource'
   - Planned: Detect if food/resource/ideological/territorial

2. **Multi-Crisis Interventions**
   - Currently: One intervention per geopolitical crisis
   - Planned: Can attempt resolution of multiple conflicts

3. **Learning Curve**
   - Currently: Fixed +2% success rate per success
   - Planned: Diminishing returns, specialization by crisis type

4. **International Coalition Mechanics**
   - Currently: Boolean multiNationOversight
   - Planned: Track which nations participate, coalition strength

5. **Adversarial Diplomacy Scenario**
   - Currently: Risk tracked but no specific outcome
   - Planned: Full slow-takeover scenario via diplomatic capture

## Related Documentation

- **Conflict Resolution:** [conflict-resolution.md](./conflict-resolution.md)
- **Catastrophic Scenarios:** [catastrophic-scenarios.md](../advanced/catastrophic-scenarios.md)
- **AI Capabilities:** [ai-agents.md](./ai-agents.md)
- **Governance Quality:** [governance-quality.md](./governance-quality.md)

---

**Last Updated:** October 9, 2025  
**Version:** 1.0 (Initial Implementation)  
**Research Foundation:** Schelling (1960), Fearon (1995), Powell (2006), Bostrom (2014), Dafoe (2020)

